model:
  # Predefined parameter configurations (≈ parameter counts)
  size:
    1m:   {dim: 128,  layers: 4,  heads: 4}   # ≈ 1 M params
    5m:   {dim: 256,  layers: 6,  heads: 8}   # ≈ 5 M (default)
    20m:  {dim: 384,  layers: 8,  heads: 8}   # ≈ 20 M
    70m:  {dim: 512,  layers: 12, heads: 8}   # ≈ 70 M
    200m: {dim: 768,  layers: 16, heads: 12}  # ≈ 200 M
    500m: {dim: 1024, layers: 24, heads: 16}  # ≈ 500 M

  # Which of the above sizes to use when the user does *not* supply
  # --size on the command-line.  Must be a key present under *sizes*.
  default: 5m

training:
  epochs: 20
  ctx: 64
  imag_h: 15
  gamma: 0.99
  lam_return: 0.95
  lam: 0.1
  online_steps: 4096
  # New configurable hyper-parameters --------------------------------
  # Mini-batch size used during gradient updates.
  batch_size: 256

  # Capacity of the *task-local* replay buffer created for each Atari
  # game.  The same value is also used for the asynchronous pre-fill
  # buffer that is populated while the previous task is still
  # training.
  replay_size: 30000

  # Fraction of samples that are drawn from the *cross-task* rehearsal
  # memory when creating a batch via ``Replay.sample_mixed``.
  sample_ratio: 0.2

  evaluation_interval: 10
